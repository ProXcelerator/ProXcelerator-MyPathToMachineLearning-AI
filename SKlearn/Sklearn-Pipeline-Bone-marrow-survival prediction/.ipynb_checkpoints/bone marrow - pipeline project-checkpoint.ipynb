{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b00ec24-99ac-4394-b3c5-b04826e89f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.io import arff\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feabf33-e641-41bd-a4ac-78db573c9895",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62a69430-42a6-4920-a51b-913ab7d2f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data = arff.loadarff('bone-marrow.arff.txt')\n",
    "df = pd.DataFrame(data[0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401e961-f8c9-4095-80e7-244f6172c887",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a8a194c4-49f6-46a3-af92-8a8b01d1c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique values in each column:\n",
      "Recipientgender           2\n",
      "Stemcellsource            2\n",
      "Donorage                187\n",
      "Donorage35                2\n",
      "IIIV                      2\n",
      "Gendermatch               2\n",
      "DonorABO                  4\n",
      "RecipientABO              4\n",
      "RecipientRh               2\n",
      "ABOmatch                  2\n",
      "CMVstatus                 4\n",
      "DonorCMV                  2\n",
      "RecipientCMV              2\n",
      "Riskgroup                 2\n",
      "Txpostrelapse             2\n",
      "Diseasegroup              2\n",
      "HLAmatch                  4\n",
      "HLAmismatch               2\n",
      "Antigen                   4\n",
      "Alel                      5\n",
      "HLAgrI                    7\n",
      "Recipientage            125\n",
      "Recipientage10            2\n",
      "Recipientageint           3\n",
      "Relapse                   2\n",
      "aGvHDIIIIV                2\n",
      "extcGvHD                  2\n",
      "CD34kgx10d6             183\n",
      "CD3dCD34                182\n",
      "CD3dkgx10d8             163\n",
      "Rbodymass               130\n",
      "ANCrecovery              18\n",
      "PLTrecovery              50\n",
      "time_to_aGvHD_III_IV     28\n",
      "survival_time           174\n",
      "survival_status           2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['Disease'], inplace=True)\n",
    "\n",
    "\n",
    "#Convert all columns to numeric, coerce errors to null values\n",
    "for c in df.columns:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    \n",
    "#Make sure binary columns are encoded as 0 and 1\n",
    "for c in df.columns[df.nunique()==2]:\n",
    "    df[c] = (df[c]==1)*1.0\n",
    "\n",
    "print('Count of unique values in each column:')\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc8373-c3f0-4d12-87c9-8eaa536ff6e4",
   "metadata": {},
   "source": [
    "# Create Feature and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1818e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set target, survival_status,as y; features (dropping survival status and time) as X\n",
    "y = df['survival_status']\n",
    "x = df.drop(columns = ['survival_status', 'survival_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab196212-0fdc-44e1-911b-8d2b411fc357",
   "metadata": {},
   "source": [
    "## Seperate numeric columns from categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ddc5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define lists of numeric and categorical columns based on number of unique values\n",
    "num_cols = x.columns[x.nunique() < 7]\n",
    "cat_cols = x.columns[x.nunique() >= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "765c1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Print columns with missing values\n",
    "nan_cols = x.isnull().sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13c0eb-f205-409f-844a-fbabf0c212f5",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "efbae79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 5. Split data into train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= .2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e2c85-65e9-4927-8d98-edf8c7cb4b21",
   "metadata": {},
   "source": [
    "# Create Pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ce4a6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create categorical preprocessing pipeline\n",
    "# Using mode to fill in missing values and OHE\n",
    "cat_vals = Pipeline([(\"imputer\", SimpleImputer(strategy= 'most_frequent')),\n",
    "                     ('OHE', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'))])\n",
    "\n",
    "# 7. Create numerical preprocessing pipeline\n",
    "# Using mean to fill in missing values and standard scaling of features\n",
    "num_vals = Pipeline([('imputer', SimpleImputer(strategy= 'mean')), ('scaler', StandardScaler())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c4dec-3d5d-435d-b47a-024cc0156972",
   "metadata": {},
   "source": [
    "## Preprocess categorical pipeline and numerical pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c701fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create column transformer that will preprocess the numerical and categorical features separately\n",
    "preprocess = ColumnTransformer(transformers= [\n",
    "                          ('cat_preprocess', cat_vals, cat_cols),\n",
    "                          ('num_preprocess', num_vals, num_cols)\n",
    "                          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be92ff-b6cd-40d4-9097-4abb64b1cb0e",
   "metadata": {},
   "source": [
    "# Build Pipeline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "305fb3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "# 9. Create a pipeline with preprocess, PCA, and a logistic regresssion model\n",
    "pipeline = Pipeline([('preprocess', preprocess), ('pca',PCA()), ('LR',LogisticRegression(random_state=0))])\n",
    "\n",
    "# 10. Fit the pipeline on the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "#Predict the pipeline on the test data\n",
    "pipeline.predict(x_test)\n",
    "print(pipeline.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bfe63-14e8-4c19-a9e6-b62d8dac9751",
   "metadata": {},
   "source": [
    "# Build Pipeline Model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04613393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "# 9. Create a pipeline with preprocess, PCA, and a logistic regresssion model\n",
    "pipeline = Pipeline([('preprocess', preprocess), ('pca',PCA()), ('RFC',RandomForestClassifier(random_state=4))])\n",
    "\n",
    "# 10. Fit the pipeline on the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "#Predict the pipeline on the test data\n",
    "pipeline.predict(x_test)\n",
    "print(pipeline.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4e907-408e-4fec-8682-5f17dd15e4c8",
   "metadata": {},
   "source": [
    "# Build Pipeline Model - Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b86c62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.020226111111111056\n"
     ]
    }
   ],
   "source": [
    "# 9. Create a pipeline with preprocess, PCA, and a logistic regresssion model\n",
    "pipeline = Pipeline([('preprocess', preprocess), ('pca',PCA()), ('RFR',RandomForestRegressor(random_state=5))])\n",
    "\n",
    "# 10. Fit the pipeline on the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "#Predict the pipeline on the test data\n",
    "pipeline.predict(x_test)\n",
    "print(pipeline.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527da884-1e61-4a91-ac2b-f28a446c37ce",
   "metadata": {},
   "source": [
    "# Tune Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7247171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define search space of hyperparameters\n",
    "search_space = [{'RFR':[RandomForestRegressor()],\n",
    "                 'RFR__max_depth': np.linspace(2,10,1).astype(int),\n",
    "                'pca__n_components':np.linspace(30,37,1).astype(int),\n",
    "                     'preprocess__num_preprocess__scaler':[MinMaxScaler(), RobustScaler(), StandardScaler()],\n",
    "                'preprocess__cat_preprocess__imputer__strategy': ['mean', 'most_frequent', 'median', 'constant'],\n",
    "                'preprocess__num_preprocess__imputer__strategy': ['mean', 'most_frequent', 'median', 'constant']}\n",
    "                \n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84e1812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 11. Define search space of hyperparameters\n",
    "# search_space = [{'RFC':[RandomForestClassifier()],\n",
    "#                  'RFC__max_depth': np.linspace(2,10,1).astype(int),\n",
    "#                 'pca__n_components':np.linspace(30,37,1).astype(int),\n",
    "#                      'preprocess__num_preprocess__scaler':[MinMaxScaler(), RobustScaler(), StandardScaler()],\n",
    "#                 'preprocess__cat_preprocess__imputer__strategy': ['mean', 'most_frequent', 'median', 'constant']}\n",
    "#                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23c3023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 11. Define search space of hyperparameters\n",
    "# search_space = [{'LR':[LogisticRegression()],\n",
    "#                      'LR__random_state': np.linspace(0,3,1).astype(int),\n",
    "#                      'LR__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#                      'LR__C': np.logspace(-4, 2, 10),\n",
    "#                 'pca__n_components':np.linspace(4,37,3).astype(int)\n",
    "# #                      'preprocess__num_preprocess__scaler':[MinMaxScaler(), RobustScaler(), StandardScaler()],\n",
    "# #                 'preprocess__cat_preprocess__imputer__strategy': ['mean', 'most_frequent', 'median', 'constant'],\n",
    "# #                 'preprocess__num_preprocess__imputer__strategy': ['mean', 'most_frequent', 'median', 'constant']\n",
    "#                 }\n",
    "                \n",
    "#                    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad2389-9e0b-4716-9754-c4967dcb8420",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0f6be993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.21544346900318823, random_state=0)\n",
      "PCA(n_components=20)\n",
      "ColumnTransformer(transformers=[('cat_preprocess',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('OHE',\n",
      "                                                  OneHotEncoder(drop='first',\n",
      "                                                                handle_unknown='ignore',\n",
      "                                                                sparse_output=False))]),\n",
      "                                 Index(['Donorage', 'HLAgrI', 'Recipientage', 'CD34kgx10d6', 'CD3dCD34',\n",
      "       'CD3dkgx10d8', 'Rbodymass', 'ANCrecovery', 'PLTrecovery',\n",
      "       'time_to_aGvHD_III_IV'],\n",
      "      dtype...\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 Index(['Recipientgender', 'Stemcellsource', 'Donorage35', 'IIIV',\n",
      "       'Gendermatch', 'DonorABO', 'RecipientABO', 'RecipientRh', 'ABOmatch',\n",
      "       'CMVstatus', 'DonorCMV', 'RecipientCMV', 'Riskgroup', 'Txpostrelapse',\n",
      "       'Diseasegroup', 'HLAmatch', 'HLAmismatch', 'Antigen', 'Alel',\n",
      "       'Recipientage10', 'Recipientageint', 'Relapse', 'aGvHDIIIIV',\n",
      "       'extcGvHD'],\n",
      "      dtype='object'))])\n",
      "0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "#12. Search over hyperparameters abolve to optimize pipeline and fit\n",
    "gs = GridSearchCV(pipeline, search_space, cv=2)\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "# 13. Save the best estimator from the gridsearch and print attributes and final accuracy on test set\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "\n",
    "# 14. Print attributes of best_model\n",
    "print(best_model.named_steps['LR'])\n",
    "print(best_model.named_steps['pca'])\n",
    "print(best_model.named_steps['preprocess'])\n",
    "# print(best_model.named_steps['RFR'])\n",
    "\n",
    "# 15. Print final accuracy score \n",
    "print(best_model.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5897e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
